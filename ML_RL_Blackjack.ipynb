{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PokerAgent\n",
    "The following code is the agent of Poker Game. There are two different way to draw cards.<br>\n",
    "(1)  Uniform:  Each card has the same probability.<br>\n",
    "(2) Optional:  The cards with 10 points have less probability.<p>\n",
    "There are three different action.<br>\n",
    "<b>Hit Stand Double<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     123
    ]
   },
   "outputs": [],
   "source": [
    "# Envrionment\n",
    "# The code is form gym PokerAgent example\n",
    "# Chin-Wei Wang add function double and split card 2019/5/22\n",
    "import random\n",
    "import numpy as np\n",
    "# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "# 卡牌點數\n",
    "deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "p=0.09\n",
    "probability = [p,p,p,p,p,p,p,p,p,p,0.10,0,0]\n",
    "\n",
    "# 比大小\n",
    "def cmp(a, b):\n",
    "    # when a > b return 1; when a < b return -1;when a == b return 0\n",
    "    return int((a > b)) - int((a < b))\n",
    "\n",
    "# 給牌\n",
    "def draw_card_uniform():\n",
    "    return random.sample(deck, 1)[0]\n",
    "\n",
    "def draw_card_non_uniform():\n",
    "    return np.random.choice(deck, 1, p=probability)[0]\n",
    "\n",
    "# Ace 當1或11\n",
    "# return true if the ace is usable\n",
    "def usable_ace(hand):\n",
    "    return 1 in hand and sum(hand) + 10 <= 21\n",
    "\n",
    "# 算牌\n",
    "def sum_hand(hand):\n",
    "    if usable_ace(hand):\n",
    "        return sum(hand) + 10\n",
    "    return sum(hand)\n",
    "\n",
    "# 爆掉\n",
    "def is_bust(hand):\n",
    "    return sum_hand(hand) > 21\n",
    "\n",
    "# 算分數\n",
    "def score(hand):\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "# 直接21點\n",
    "def is_natural(hand):\n",
    "    return sorted(hand) == [1, 10]\n",
    "\n",
    "# 判斷兩張相同時可分牌\n",
    "def is_duplicated(hand):\n",
    "    if np.size(hand) == 2:\n",
    "        return hand[0] == hand[1]\n",
    "    return False\n",
    "\n",
    "# action(0, 1, 2)     -> (stay, hit, double)\n",
    "# action(0, 1, 2, 3) -> (stay, hit, double, split)\n",
    "\n",
    "class PokerAgent:\n",
    "    def __init__(self, natural=True):\n",
    "        self.action_size = 1\n",
    "        self.state_size = [1, 3]\n",
    "        self.natural = natural\n",
    "        self.uniform = True\n",
    "        self.duplicated = False\n",
    "        self.Surrender = False\n",
    "        self.player = []\n",
    "        self.dealer = []\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Split\n",
    "        if is_duplicated(self.player) and self.duplicated:\n",
    "            duplicate = self.player.pop(1)\n",
    "            if self.uniform:\n",
    "                self.player.append(draw_card_uniform())\n",
    "            else:\n",
    "                self.player.append(draw_card_non_uniform())\n",
    "            done = True\n",
    "            \n",
    "            rewards = 0\n",
    "            if is_bust(self.player):   # 第一組牌\n",
    "                reward = -1\n",
    "            else:\n",
    "                while sum_hand(self.dealer) < 17:\n",
    "                    if self.uniform:\n",
    "                        self.dealer.append(draw_card_uniform())\n",
    "                    else:\n",
    "                        self.dealer.append(draw_card_non_uniform())\n",
    "                reward = cmp(score(self.player), score(self.dealer))     \n",
    "            \n",
    "            self.player=[]\n",
    "            self.player.append(duplicate)\n",
    "            \n",
    "            if self.uniform:\n",
    "                self.player.append(draw_card_uniform())\n",
    "            else:\n",
    "                self.player.append(draw_card_non_uniform())\n",
    "                \n",
    "            if is_bust(self.player):   # 第二組牌\n",
    "                reward = reward-1\n",
    "            else:\n",
    "                while sum_hand(self.dealer) < 17:\n",
    "                    if self.uniform:\n",
    "                        self.dealer.append(draw_card_uniform())\n",
    "                    else:\n",
    "                        self.dealer.append(draw_card_non_uniform())\n",
    "                reward = cmp(score(self.player), score(self.dealer)) + reward\n",
    "        else:\n",
    "            # stand\n",
    "            if action == 0:\n",
    "                done = True\n",
    "                # dealer's turn\n",
    "                while sum_hand(self.dealer) < 17:\n",
    "                    if self.uniform:\n",
    "                        self.dealer.append(draw_card_uniform())\n",
    "                    else:\n",
    "                        self.dealer.append(draw_card_non_uniform())\n",
    "                reward = cmp(score(self.player), score(self.dealer))\n",
    "                if self.natural and is_natural(self.player) and reward == 1:\n",
    "                    reward = 1.5\n",
    "\n",
    "            # hit\n",
    "            elif action == 1:\n",
    "                if self.uniform:\n",
    "                    self.player.append(draw_card_uniform())\n",
    "                else:\n",
    "                    self.player.append(draw_card_non_uniform())               \n",
    "\n",
    "                if is_bust(self.player):\n",
    "                    done = True\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    done = False\n",
    "                    reward = 0\n",
    "\n",
    "            # double down\n",
    "            elif action == 2:\n",
    "                if self.uniform:\n",
    "                    self.player.append(draw_card_uniform())\n",
    "                else:\n",
    "                    self.player.append(draw_card_non_uniform()) \n",
    "                done = True\n",
    "                if is_bust(self.player):\n",
    "                    reward = -1 * 2\n",
    "                # dealer's turn\n",
    "                else:\n",
    "                    while sum_hand(self.dealer) < 17:\n",
    "                        if self.uniform:\n",
    "                            self.dealer.append(draw_card_uniform())\n",
    "                        else:\n",
    "                            self.dealer.append(draw_card_non_uniform())\n",
    "                    reward = cmp(score(self.player), score(self.dealer)) * 2\n",
    "        \n",
    "            # Surrender\n",
    "            else:\n",
    "                done = True\n",
    "                if self.natural and is_natural(self.player):\n",
    "                    reward = 1.5\n",
    "                else:\n",
    "                    reward = -0.5\n",
    "                \n",
    "        return self.get_obs(), reward, done, {}\n",
    "\n",
    "    def get_obs(self):\n",
    "        return [sum_hand(self.player), self.dealer[0], usable_ace(self.player)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = []\n",
    "        self.dealer = []\n",
    "        if self.uniform:\n",
    "            self.dealer.append(draw_card_uniform())\n",
    "            self.player.append(draw_card_uniform())\n",
    "            self.player.append(draw_card_uniform())\n",
    "        else:\n",
    "            self.dealer.append(draw_card_non_uniform())\n",
    "            self.player.append(draw_card_non_uniform())\n",
    "            self.player.append(draw_card_non_uniform())\n",
    "        return self.get_obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Policy\n",
    "In geneal, when the total points of hand cards reach to 17, we don't ask card anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "# action(0, 1, 2) -> (stay, hit, double)\n",
    "def human_policy(env):\n",
    "    if sum_hand(env.player) < 17 and sum(env.dealer) > 8:\n",
    "        action = 1   # double\n",
    "    elif sum_hand(env.player) < 17:\n",
    "        action = 1   # hit\n",
    "    else:\n",
    "        action = 0  # stay\n",
    "    observation,reward,done,_ = env.step(action)\n",
    "    return action,observation,reward,done\n",
    "\n",
    "def random_policy(env):\n",
    "    action = np.random.randint(0,4)\n",
    "    observation,reward,done,_ = env.step(action)\n",
    "    return action,observation,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_card(i_time,env):\n",
    "    print(f\"Game Set : {i_time}\")\n",
    "    print(f\"Player Card : {env.player}\")\n",
    "    print(f\"Is Bust : {is_bust(env.player)}\")\n",
    "    print(f\"Dealer Card : {env.dealer}\")\n",
    "\n",
    "def print_result(i_times,actions,results,money_):\n",
    "    print(f\"Total : {i_times} times\")\n",
    "    print(f\"stay : {actions.count(0)} , hit : {actions.count(1)} , double : {actions.count(2)}\")\n",
    "    print(f\"Player win: {results.count(2)+results.count(1.5)+results.count(1)} Player double win: {results.count(2)}\")\n",
    "    print(f\"Dealer win: {results.count(-1)+results.count(-2)} Dealer double win: {results.count(-2)}\")\n",
    "    print(f\"Tie : {results.count(0)}\")\n",
    "    print(f\"Money : {money_} \")\n",
    "    print(f\"Expection value : {money_/i_times} \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Human Policy Game Loop\n",
    "There are two different policy.<br>\n",
    "(1) Random Policy<br>\n",
    "(2) Human Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "each_print = False\n",
    "# Play the game\n",
    "def game_loop(game_times,step_times):\n",
    "    env = PokerAgent()\n",
    "    results = []\n",
    "    actions = []\n",
    "    money = []\n",
    "    money_ = 0\n",
    "    for i in range(1,game_times+1):\n",
    "        for j in range(step_times):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while done == False:\n",
    "                action,observation,reward,done = human_policy(env)\n",
    "                #action,observation,reward,done = random_policy(env)\n",
    "\n",
    "                actions.append(action)\n",
    "            results.append(reward)\n",
    "            money_ += reward\n",
    "            money.append(money_)\n",
    "        if each_print and i % step_times == 0:\n",
    "             print_result(i*step_times,actions,results,money_)\n",
    "\n",
    "    print_result(i*step_times,actions,results,money_)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.arange(1,game_times),money[1:game_times])\n",
    "    plt.ylabel('Money')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total : 100000 times\n",
      "stay : 71946 , hit : 90456 , double : 0\n",
      "Player win: 40754 Player double win: 0\n",
      "Dealer win: 48786 Dealer double win: 0\n",
      "Tie : 10460\n",
      "Money : -5986.5 \n",
      "Expection value : -0.059865 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "game_loop(100000,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = 64\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.95         # discount rate\n",
    "        self.epsilon = 1.0        # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def replay(self):\n",
    "        minibatch = self.sample(self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.target_model.predict(next_state)[0]))           \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def update_model(self, total_step):\n",
    "          if total_step % 10 == 0:      # %10\n",
    "                weights = self.model.get_weights()\n",
    "                target_weights = self.target_model.get_weights()\n",
    "                for i in range(len(target_weights)):\n",
    "                    target_weights[i] = weights[i]\n",
    "                self.target_model.set_weights(target_weights)\n",
    "                \n",
    "    def save(self, file):\n",
    "        self.model.save_weights(file)\n",
    "\n",
    "    def load(self, file):\n",
    "        self.model.load_weights(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "# Double Deep Q-learning Agent\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = 64\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.95         # discount rate\n",
    "        self.epsilon = 1.0        # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def replay(self):\n",
    "        minibatch = self.sample(self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                predicted_action = np.argmax(self.model.predict(next_state)[0])                \n",
    "                target = reward + self.gamma * self.target_model.predict(next_state)[0][predicted_action]         \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def update_model(self, total_step):\n",
    "          if total_step % 10 == 0:      # %10\n",
    "                weights = self.model.get_weights()\n",
    "                target_weights = self.target_model.get_weights()\n",
    "                for i in range(len(target_weights)):\n",
    "                    target_weights[i] = weights[i]\n",
    "                self.target_model.set_weights(target_weights)\n",
    "                \n",
    "    def save(self, file):\n",
    "        self.model.save_weights(file)\n",
    "\n",
    "    def load(self, file):\n",
    "        self.model.load_weights(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = PokerAgent()\n",
    "state_size_,action_size_ = 3, 4    # 3,4\n",
    "#dqn_agent = DQNAgent(state_size_, action_size_)\n",
    "dqn_agent = DDQNAgent(state_size_, action_size_)\n",
    "# dqn_agent.load('poke_model_per_d.h5')\n",
    "n_episode = 15000\n",
    "n_steps = 10\n",
    "stand, hit, double, surrender = 0, 0, 0, 0\n",
    "bj_win = 0\n",
    "money = 0\n",
    "capacity = 10000\n",
    "cur_money = money\n",
    "total_step = 0\n",
    "mon = []\n",
    "# start training\n",
    "for episode_i in range(n_episode):\n",
    "    cur_state_ = env.reset()\n",
    "    for step_i in range(n_steps):\n",
    "        total_step += 1\n",
    "        cur_state_ = np.reshape(cur_state_, [1, dqn_agent.state_size])\n",
    "        if total_step > capacity:\n",
    "            action = dqn_agent.act(cur_state_)\n",
    "        else:\n",
    "            action = np.random.randint(0, 4)    # (0,3)\n",
    "        if action == 0:\n",
    "            stand += 1\n",
    "        elif action == 1:\n",
    "            hit += 1\n",
    "        elif action == 2:\n",
    "            double += 1\n",
    "        elif action ==3:\n",
    "            surrender +=1\n",
    "            \n",
    "        observation, reward_, done_, _ = env.step(action)\n",
    "        observation = np.reshape(observation, [1, dqn_agent.state_size])\n",
    "        dqn_agent.remember(cur_state_, action, reward_ , observation, done_)\n",
    "        if total_step > capacity:\n",
    "            dqn_agent.replay()\n",
    "            dqn_agent.update_model(total_step)\n",
    "        cur_state_ = observation\n",
    "        if done_:\n",
    "            if reward_ == 1.5:\n",
    "                bj_win += 1\n",
    "            money += reward_\n",
    "            break\n",
    "\n",
    "    if episode_i % 100 == 0:\n",
    "        money_change = money - cur_money\n",
    "        mon.append(money_change)\n",
    "        dqn_agent.save('poke_model_dd_u_2.h5')\n",
    "        print(f\"stand {stand}\")\n",
    "        print(f\"hit {hit}\")\n",
    "        print(f\"double {double}\")\n",
    "        print(f\"surrender {surrender}\")\n",
    "\n",
    "        stand, hit, double, surrender = 0, 0, 0, 0\n",
    "        print(\"episode {}/{}, money_change:{}, e:{}\".format(episode_i, n_episode, money_change, dqn_agent.epsilon))\n",
    "        cur_money = money\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = np.linspace(0,15000,num = 150)\n",
    "plt.plot(ax,mon)\n",
    "plt.title('Rewards',fontsize=12)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(money)\n",
    "print(bj_win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Game Loop\n",
    "The following code is using the Policy after Q-Learning.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from note import draw_graph\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = PokerAgent()\n",
    "state_size_, action_size_ = 3, 3    # 3,3\n",
    "#dqn_agent = DQNAgent(state_size_, action_size_)\n",
    "dqn_agent = DDQNAgent(state_size_, action_size_)\n",
    "\n",
    "dqn_agent.load('poke_model_dd_u_1.h5') # Uniform DQNAgent\n",
    "\n",
    "\n",
    "dqn_agent.epsilon  = 0\n",
    "n_episode = 100000\n",
    "n_steps = 1\n",
    "stand, hit, double, surrender, bj_win = 0, 0, 0, 0, 0\n",
    "money = 0\n",
    "cur_money = money\n",
    "game = []\n",
    "moneys = []\n",
    "for episode_i in range(n_episode):\n",
    "    cur_state_ = env.reset()\n",
    "    for step_i in range(n_steps):\n",
    "        cur_state_ = np.reshape(cur_state_, [1, dqn_agent.state_size])\n",
    "        action = dqn_agent.act(cur_state_)\n",
    "\n",
    "        if action == 0:\n",
    "             stand += 1\n",
    "        elif action == 1:\n",
    "             hit += 1\n",
    "        elif action == 2:\n",
    "             double += 1\n",
    "        elif action == 3:\n",
    "             surrender +=1\n",
    "                \n",
    "        observation, reward_, done_, _ = env.step(action)\n",
    "        observation = np.reshape(observation, [1, dqn_agent.state_size])\n",
    "        cur_state_ = observation\n",
    "        if done_:\n",
    "            if reward_ == 1.5:\n",
    "                bj_win += 1\n",
    "            money += reward_\n",
    "            break\n",
    "    if episode_i % 1000 == 0:\n",
    "        money_change = money - cur_money\n",
    "        game.append(money_change)\n",
    "        cur_money = money\n",
    "        moneys.append(money)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Result\n",
    "Show the result with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Stand Rate : {stand / (stand + hit + double+surrender)}\")\n",
    "print(f\"Hit Rate : {hit / (stand + hit + double+surrender)}\")\n",
    "print(f\"Double Rate : {double / (stand + hit + double+surrender)}\")\n",
    "print(f\"Surrender Rate : {surrender / (stand + hit + double+surrender)}\")\n",
    "\n",
    "\n",
    "print(f\"Total Times  : {stand + hit + double+surrender}\")\n",
    "print(f\"Money : {money}\")\n",
    "print(f\"BJ Win : {bj_win}\")\n",
    "print(f\"Expect Value : {money/(stand + hit + double+surrender+bj_win)}\")\n",
    "print(f\"Expect Value : {money/(100000)}\")\n",
    "\n",
    "ax = np.linspace(0,100000,num = 100)\n",
    "plt.plot(ax,game)\n",
    "plt.title('Game per 1000',fontsize=12)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(ax,moneys)\n",
    "plt.title('Total Money',fontsize=12)\n",
    "plt.grid()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
